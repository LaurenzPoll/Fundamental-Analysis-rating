{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"pandas version: \", pd.__version__)\n",
    "print(\"seaborn version:\", sns.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Requirements\n",
    "This data dictionary is for all data that will be used for our model. We have taken this out of the data requirement document (this can be found in this folder under the name data_requirements.docx), A changelog will be kept in the data requirement document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "| Dutch Term              | English Term             | Description                                                        | Data Type      | Range                        | Units          | Source          | Quality Standards          |\n",
    "|-------------------------|--------------------------|--------------------------------------------------------------------|----------------|------------------------------|----------------|-----------------|----------------------------|\n",
    "| Datum                   | Date                     | The date when the analyst ratings were recorded.                   | Date           | 2022-07-14 to 2022-12-01     | Date           | Marketscreener     | Accurate date required     |\n",
    "| Kopen                   | Buy                      | Number of 'Buy' ratings from analysts.                             | Numerical      | 0 - 15                       | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Overpresteren           | Outperform               | Number of 'Outperform' ratings from analysts.                      | Numerical      | 0 - 15                       | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Houden                  | Hold                     | Number of 'Hold' ratings from analysts.                            | Numerical      | 0 - 15                       | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Onderpresteren          | Underperform             | Number of 'Underperform' ratings from analysts.                    | Numerical      | 0 - 15                            | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Verkopen                | Sell                     | Number of 'Sell' ratings from analysts.                            | Numerical      | 0 - 15                            | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Datum | Date         | The date when the stock market data was recorded.                           | Date       | Varied (min 11-06-2016 etc.)  | Date     | Yahoo Finance| Accurate date required   |\n",
    "| Open | Open         | Opening price of the stock for the given date.                              | Numerical  | Varied (min 0,.-max ∞)                    | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Hoog | High         | Highest price of the stock reached on the given date.                       | Numerical  | Varied (min 0,.-max ∞)                   | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Laag | Low          | Lowest price of the stock on the given date.                                | Numerical  | Varied (min 0,.-max ∞)                   | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Slot | Close        | Closing price of the stock at the end of the trading day on the given date. | Numerical  | Varied (min 0,.-max ∞)                   | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Aangpaste slot | Adj Close    | Adjusted closing price after adjustments for all applicable splits and dividend distributions.| Numerical | Varied (min 0,.-max ∞) | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Volume | Volume       | Number of shares of the stock traded during the given date.                 | Numerical  | Varied (min 0,.-max ∞)                   | Shares   | Yahoo Finance| Accurate count           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Analyst consensus**: Extracted by typing over the data from an interactive graph, Currently there are no free sources to get this data.\n",
    "- **Financial data from ASR**: Looked through the financials from ASR on their website, then extracted the neccessary data by typing it over in a new excel file.\n",
    "- **Stock data for ASR**: On the Yahoo website there is a page to download the data by Day, Month or Year. The data range can be stated before downloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed process\n",
    "We will explain exactly how we collected the data so these steps can be replicated.\n",
    "\n",
    "#### Analyst consensus\n",
    "- **Source**: [marketscreener.com](https://www.marketscreener.com/quote/stock/ASR-NEDERLAND-N-V-28377340/consensus/)\n",
    "- **Limitation**: Not downloadable data, only up to 18 months of history.\n",
    "- **Time Frame**: Data from 2022 July up to 2023 Januari. With a moving time frame(Only 18 months back from current date)\n",
    "- **Script**: No script used\n",
    "- **Storage**: Data stored locally in the data folder. Filename: [analyst_consensus_16_07-2022_17-01-2024.xlsx](data/analyst_consensus_16_07-2022_17-01-2024.xlsx).\n",
    "- **Future Data Addition**: Look for a source with better historical data that is easier to gather\n",
    "\n",
    "\n",
    "#### Financial data from asr\n",
    "- **Source**: [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)\n",
    "- **Limitation**: The report are per half year or full year. The best would be that a shorter time frame is available. This is not likely to happen \n",
    "- **Time Frame**: Financial data available from 2016 up to 2023. (asr whent public in 2016 June)\n",
    "- **Script**: No script used. Downloaded the 'Tables' and 'Financial ratios' under column 'Halfjaarcijfers' in the tab from '2023'. These 2 files contain the data for 2022 and first half of 2023, last half is not released yet.\n",
    "- **Storage**: Data stored locally in the data folder. Filename: [extracted_financial_data_HY2022_FY2022_HY2023.xlsx](data\\extracted_financial_data_HY2022_FY2022_HY2023.xlsx).\n",
    "The original data from the asr website is stored in the folder named 'Original data asr'\n",
    "- **Future Data Addition**: When newer reports are released these should be downloaded and the data should be added to [extracted_financial_data_HY2022_FY2022_HY2023.xlsx](data\\extracted_financial_data_HY2022_FY2022_HY2023.xlsx).\n",
    "\n",
    "---\n",
    "- **Important note**: We collected the data according to the new calculations that went into use in 2023. asr has recalculated the values for 2022. This is the statement at the top of their books `all 2022 IFRS figures restated to IFRS17 / 9`<br>**Due to this we won't be using this data for iteration 0**\n",
    "---\n",
    "\n",
    "#### Stock data for asr\n",
    "- **Source**: [finance.yahoo.com](https://finance.yahoo.com/quote/ASRNL.AS/history)\n",
    "- **Limitation**: \n",
    "- **Time Frame**: Financial data available from 2016 up to 2023. (asr whent public in 2016 July)\n",
    "- **Script**: No script used. Under the tab history there is the option to download the data from a given date as daily, monthly or yearly data. The download is in a csv format.\n",
    "- **Storage**: Data stored locally in the data folder. We downloaded the full data available in three files\n",
    "  - Daily: Filename: [StockPerDay_ASRNL.AS_01-01-2016_19-01-2024.csv](data/StockPerDay_ASRNL.AS_01-01-2016_19-01-2024.csv)\n",
    "  - Weekly: Filename: [StockPerWeek_ASRNL.AS_01-01-2016_19-01-2024.csv](data/StockPerWeek_ASRNL.AS_01-01-2016_19-01-2024.csv)\n",
    "  - Yearly: Filename: [StockPerYear_ASRNL.AS_01-01-2016_19-01-2024.csv](data/StockPerYear_ASRNL.AS_01-01-2016_19-01-2024.csv)\n",
    "- **Future Data Addition**: When newer data is available for `Analyst consensus` or `Financial data from ASR` then from the yahoo source the new stock data should be downloaded and replace the existing files for daily, monthly and yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling\n",
    "All data handling, including preprocessing and cleaning, will be conducted within this notebook, focusing on highways in the Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding & Preparation for analyst consensus\n",
    "In this part we will look at the data for the analyst consensus. This will be the most important data for our target variable.\n",
    "\n",
    "We will start with loading the data and taking a first look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalystConsensus = pd.read_excel('data/analyst_consensus_16_07-2022_17-01-2024.xlsx')\n",
    "AnalystConsensus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded correctly. We can see from this;\n",
    "- one column to indicate the date\n",
    "- 5 columns to show the count of the analyst consensus\n",
    "- There seems to be around two weeks between every entry\n",
    "\n",
    "We will now describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalystConsensus.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know there are 37 entries into the dataset.\n",
    "The newest datapoint is from 17-01-2024 and the oldest is from 14-07-2022.\n",
    "\n",
    "What is also visible is that the max count of the consensus fields is in all cases below 9. From this we can see that at maximum there are not a lot of analyst giving their opinion about asr\n",
    "\n",
    "Now we will look if the types are correct with the .info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalystConsensus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that everything has the correct datatype and there are no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "Below we will create an interactive plot to visualize the data in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = AnalystConsensus\n",
    "\n",
    "# Initialize go\n",
    "fig = go.Figure()\n",
    "\n",
    "# add bars\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Sell'],\n",
    "    name='Sell',\n",
    "    marker_color='red'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Underperform'],\n",
    "    name='Underperform',\n",
    "    marker_color='orange'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Hold'],\n",
    "    name='Hold',\n",
    "    marker_color='yellow'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Outperform'],\n",
    "    name='Outperform',\n",
    "    marker_color='lightgreen'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Buy'],\n",
    "    name='Buy',\n",
    "    marker_color='green'\n",
    "))\n",
    "\n",
    "# update the figure\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Analyst Consensus - Interactive Stacked Bar Chart',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Consensus Count',\n",
    "    legend_title='Consensus',\n",
    "    hovermode='x'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that analust overall think that asr is a good buy. With a slight hiccup around march where there was a sell consensus.\n",
    "\n",
    "We can also see that there is a difference in the count over the weeks, this is something to look into in the prepocessing step of Phase 3 as this can influence the machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the analyst consensus did not need any altering. The data is clear and we have a good visualization that shows the consensus over time, what we did notice it the varying count over the weeks. This is something we need to look at in the preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding & Preparation for Financial data from asr\n",
    "In this section we will look at the financial data gathered from asr.\n",
    "\n",
    "We will start with loading the data and taking a first look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinancialDataASR = pd.read_excel('data/extracted_financial_data_HY2022_FY2022_HY2023.xlsx')\n",
    "FinancialDataASR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataframe only has 3 entries.\n",
    "This is due to the reporting from asr. Every halfyear and year there is a new report.\n",
    "\n",
    "For our project this means that we need to fill in the data in between these points since this will be important for our model to understand the analyst consensus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As stated before, this is according to the new calculations. for this reason we will leave this data and not use it for iteration zero**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding & Preparation for Stock data for asr\n",
    "In this section we will look at the stock data gathered for asr.\n",
    "\n",
    "The analyst consensus data has a new entry around every two weeks. We want to have our data matching to this timeframe. For the stock data we will use the data gathered per week.\n",
    "We will start with loading the data and taking a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek = pd.read_csv('data/StockPerWeek_ASRNL.AS_01-01-2016_19-01-2024.csv', delimiter=',', decimal='.')\n",
    "StockPerWeek.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataframe columns for the date. We can see that the data is indeed weekly since every 7 days there is a new entry.\n",
    "\n",
    "The dataframe also contains data for the Open, High, Low and Volume of that week.\n",
    "\n",
    "The *Close and **Adj Close are fields with a description. This is the explanation from Yahoo;\n",
    "- `*Close price adjusted for splits.`\n",
    "- `**Adjusted close price adjusted for splits and dividend and/or capital gain distributions.`\n",
    "\n",
    "In the next step we will use .info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 397 entries for all columns and we can also see that there are no null values.\n",
    "\n",
    "for the datatypes we need to change the data from object to a date, we will do this in the code block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime\n",
    "StockPerWeek['Date'] = pd.to_datetime(StockPerWeek['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the .info above we can see that the conversion of `Date` was succesful.\n",
    "Now we will use .describe to look for anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .describe shows that the dataset appears normal, with no extreme values observed in any of the columns. One observation is that the earliest date in the dataset is June 13, 2016. This date represents more historical data than is currently required for matching with the analyst consensus.\n",
    "\n",
    "Going forward, our plan includes the integration of additional analyst consensus data. This means that we will review this entire dataset, starting from 2016. This ensures that we can effectively incorporate new consensus data as it becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "Below we will start the visualization with creating a plot to visualize the volume over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "df = StockPerWeek\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['Date'], df['Volume'], color='blue', label='Trading Volume', width=3)\n",
    "plt.title('Trading Volume Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "\n",
    "# This is needed to transform the mathemetical notation to numbers\n",
    "plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "\n",
    "# Set the limit to 8.000.000 to capture most datapoint and still have it readable\n",
    "plt.ylim(0, 8000000)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided plot, which appears to show the trading volume over time, we can observe that there is a fluctuating but somewhat consistent pattern of trade volume throughout the period from 2017 to 2024. There are notable spikes in volume at certain intervals, which could correspond to specific market events, earnings announcements, or other news affecting trading activity. However, the majority of trading days seem to have a volume well below these peaks. The consistency of the lower volume suggests a baseline level of trading activity, while the spikes indicate periods of high trader interest or market volatility. It would be interesting to cross-reference the spikes with actual market events to draw more detailed conclusions on what may have caused these surges in trading volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to see the close price over time as this gives us a good overview of the stock movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Date'], df['Close'], label='Close Price')\n",
    "plt.title('Closing Stock Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to see the two plots overlapping with each other to see if there is an effect on the close price due to trading volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the first plot for trading volume on the primary y-axis\n",
    "ax1 = plt.gca()  # Get the current Axes instance on the current figure\n",
    "ax1.bar(df['Date'], df['Volume'], color='blue', label='Trading Volume', width=3)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Volume', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "ax1.set_ylim(0, 8000000)  # Set the limit to capture most datapoints for volume\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Create a secondary y-axis for the closing price\n",
    "ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "ax2.plot(df['Date'], df['Close'], label='Close Price', color='green')\n",
    "ax2.set_ylabel('Price', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('Trading Volume and Closing Stock Prices Over Time')\n",
    "ax1.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not appear that there is a direct correlation between the stock price movement and trading volume.\n",
    "\n",
    "It is noteworthy that there are a few occasions where the stock price drops dramatically and then we can also see a massive spike in trading volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating data from Stock market and Analyst consensus\n",
    "We know from our domain understanding that the consensus is often reflected in the stock market or the other way around.\n",
    "\n",
    "In this section we will add the datasets together, we will lose a part of the stock market data by doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will add a column to both dataset to state the number of the week and one column for the year, afterards we will join them based on the week number.\n",
    "We will start with the analyst consensus dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnalystConsensus\n",
    "\n",
    "# Creating a column named Week to store the week number\n",
    "AnalystConsensus['Week'] = AnalystConsensus['Date'].dt.isocalendar().week\n",
    "\n",
    "# Creating a column named Year to store the year number\n",
    "AnalystConsensus['Year'] = AnalystConsensus['Date'].dt.year\n",
    "\n",
    "# Display the first rows to verify\n",
    "AnalystConsensus.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StockPerWeek\n",
    "\n",
    "# Creating a column named Week to store the week number\n",
    "StockPerWeek['Week'] = StockPerWeek['Date'].dt.isocalendar().week\n",
    "\n",
    "# Creating a column named Year to store the year number\n",
    "StockPerWeek['Year'] = StockPerWeek['Date'].dt.year\n",
    "\n",
    "# Display the first rows to verify\n",
    "StockPerWeek.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we added the week column we need to edit the stock market dataframe.\n",
    "\n",
    "First we will look at the max and min week + year of the analyst consensus dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum years in the DataFrame\n",
    "min_year = AnalystConsensus['Year'].min()\n",
    "max_year = AnalystConsensus['Year'].max()\n",
    "\n",
    "# Filtering AnalystConsensus to only include rows from the min year\n",
    "min_year_df = AnalystConsensus[AnalystConsensus['Year'] == min_year]\n",
    "\n",
    "# Now we look for the min week within the min year\n",
    "min_week_in_min_year = min_year_df['Week'].min()\n",
    "\n",
    "# Here we will do the same for max\n",
    "max_year_df = AnalystConsensus[AnalystConsensus['Year'] == max_year]\n",
    "max_week_in_max_year = max_year_df['Week'].max()\n",
    "\n",
    "\n",
    "print(f\"The minimum week in the minimum year {min_year} is: {min_week_in_min_year}\")\n",
    "print(f\"The maximum week in the maximum year {max_year} is: {max_week_in_max_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the min is week 28 of the year 2022 and the max is week 3 of the year 2024.\n",
    "\n",
    "We will remove all fields that do not fall inbetween these weeks from the stock market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include rows that fall within the given range\n",
    "# Include all weeks in the min_year after min_week_in_min_year\n",
    "# Include all weeks in the max_year up to and including max_week_in_max_year\n",
    "# For years between min_year and max_year, include all weeks\n",
    "\n",
    "filtered_df = StockPerWeek[\n",
    "    ((StockPerWeek['Year'] == min_year) & (StockPerWeek['Week'] >= min_week_in_min_year)) |\n",
    "    ((StockPerWeek['Year'] == max_year) & (StockPerWeek['Week'] <= max_week_in_max_year)) |\n",
    "    ((StockPerWeek['Year'] > min_year) & (StockPerWeek['Year'] < max_year))\n",
    "]\n",
    "\n",
    "# Now let's print the shape of the original and filtered DataFrames to see how many rows were removed\n",
    "print(f\"Original DataFrame shape: {StockPerWeek.shape}\")\n",
    "print(f\"Filtered DataFrame shape: {filtered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the shape we can see that changes have been made. There now should be 80 rows in the dataset. This appears to be the correct time range. We will print the max and min week + year to check if the values are the same as in the ``AnalystConsensus`` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek = filtered_df\n",
    "\n",
    "# Find the minimum and maximum years in the DataFrame\n",
    "min_year = StockPerWeek['Year'].min()\n",
    "max_year = StockPerWeek['Year'].max()\n",
    "\n",
    "# Filtering AnalystConsensus to only include rows from the min year\n",
    "min_year_df = StockPerWeek[StockPerWeek['Year'] == min_year]\n",
    "\n",
    "# Now we look for the min week within the min year\n",
    "min_week_in_min_year = min_year_df['Week'].min()\n",
    "\n",
    "# Here we will do the same for max\n",
    "max_year_df = StockPerWeek[StockPerWeek['Year'] == max_year]\n",
    "max_week_in_max_year = max_year_df['Week'].max()\n",
    "\n",
    "\n",
    "print(f\"The minimum week in the minimum year {min_year} is: {min_week_in_min_year}\")\n",
    "print(f\"The maximum week in the maximum year {max_year} is: {max_week_in_max_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that the min and max values are the same we can start joining the two dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two DataFrames on 'Year' and 'Week' using an inner join.\n",
    "merged_df = pd.merge(StockPerWeek, AnalystConsensus, on=['Year', 'Week'], how='inner')\n",
    "\n",
    "# This will result in a DataFrame that only contains rows where both 'Year' and 'Week' match in both DataFrames.\n",
    "# All weeks that do not exist in AnalystConsensus will be dropped.\n",
    "\n",
    "# Displaying the shape of the new df\n",
    "print(f\"Merged DataFrame shape: {merged_df.shape}\")\n",
    "\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the two dataframes are merged on week and year we will continue to Phase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We will collect all imports at the top of this section to keep it clear what we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the data we are working with using the .describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that date is twice in the df. We will not be using both dates so we will be dropping ``Date_y`` and renaming `Date_x` to ``Date``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Date_y' column\n",
    "merged_df.drop('Date_y', axis=1, inplace=True)\n",
    "\n",
    "# Rename 'Date_x' to 'Date'\n",
    "merged_df.rename(columns={'Date_x': 'Date'}, inplace=True)\n",
    "\n",
    "merged_df.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have dropped and renamed columns we are going to rename ``merged_df`` to ``df`` and use info to check the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All DType are correct. We also need to look at our target variable. From Phase 2 we know that the total amount switches every so often. It would be best to change these numbers to percentages based on the total count of the week. We will be doing this in the code block below step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of ratings for each row\n",
    "df['Total_Ratings'] = df[['Buy', 'Outperform', 'Hold', 'Underperform', 'Sell']].sum(axis=1)\n",
    "\n",
    "# Convert each rating to a percentage of the total\n",
    "for column in ['Buy', 'Outperform', 'Hold', 'Underperform', 'Sell']:\n",
    "    df[column + '_%'] = (df[column] / df['Total_Ratings']) * 100\n",
    "\n",
    "# Drop the original count columns and the total ratings column if they are no longer needed\n",
    "df.drop(['Buy', 'Outperform', 'Hold', 'Underperform', 'Sell', 'Total_Ratings'], axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will create a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we see is the missing values for `Underperform_%`. This is due to there being no values for this variable. This can introduce bias in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "From the correlation matrix above we can see that the `Open`, `High`, `Low`, `Close` and `Adj Close` are highly correlated. We will pick the `Close` feature .\n",
    "It seems that the ``Volume`` does not have a high correlation but we will keep it in there for iteration zero.\n",
    "\n",
    "It also seems that the ``Year`` has a high correlation to the target. Since we only have 1 full year we will not be using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['Close', 'Volume']]\n",
    "target = df[['Buy_%', 'Outperform_%', 'Hold_%', 'Underperform_%', 'Sell_%']] \n",
    "\n",
    "X = features\n",
    "y = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting in to train/test\n",
    "The data we use is time series we need to keep this in mind with splitting the data. For now we will set the split at 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set splitting point at 80%\n",
    "split_point = int(len(df) * 0.80)\n",
    "\n",
    "# Split the features and target into training and testing sets\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "# Print the sizes of the train and test sets\n",
    "print(f\"Training set size: {X_train.shape[0]} rows\")\n",
    "print(f\"Test set size: {X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "We selected a Random Forest Regressor as our baseline model to predict weekly analyst consensus ratings, which are represented as percentages. Random Forest is chosen for its ability to handle complex and varied data types effectively. It's known for its robustness, reducing overfitting by averaging multiple decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [100, 200, 300],\n",
    "    'estimator__max_depth': [None, 10, 20, 30],\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "random_forest_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Wrap the model with MultiOutputRegressor\n",
    "multi_target_regressor = MultiOutputRegressor(random_forest_regressor)\n",
    "\n",
    "# Set up the grid search with cross-validation\n",
    "grid_search = GridSearchCV(multi_target_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using multiple metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# If you want to do cross-validated performance assessment:\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-validated MSE: {-cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result that are printed do not look very promising. In the next section we will explore what happened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Here we will create plots to see where it went wrong in our model and end with the conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values for all targets\n",
    "target_names = y_test.columns\n",
    "for i, target in enumerate(target_names):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test[target], y_pred[:, i], alpha=0.5)\n",
    "    plt.title(f'Actual vs Predicted - {target}')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'k--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we can deduce that for some targets the model makes predictions even though the actual values should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In the future iterations we need to look beter at preprocessing of the target variables. The model does not understand it at this moment.\n",
    "\n",
    "We also need to look at a way to solve the bias introduced with some targets having no data points in the dataset. We could solve it by:\n",
    "\n",
    "- Instead of in the demonstration adding the buy and outperform or sell and underperform together, we could do it in the preprocessing step to create more datapoints\n",
    "- Collecting more data but this can be difficult\n",
    "- Altering class weights\n",
    "- Resampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4\n",
    "## Demonstration\n",
    "In the following section we will create the code that can be used to make a prediction for our stakeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_analyst_category(model, close, volume, target_names, feature_names):\n",
    "    \"\"\"\n",
    "    Predict the analyst category ('Buy', 'Hold', or 'Sell') based on 'Close' and 'Volume' using the trained model.\n",
    "\n",
    "    :param model: Trained MultiOutputRegressor model.\n",
    "    :param close: Adjusted close value.\n",
    "    :param volume: Trading volume.\n",
    "    :param target_names: List of target variable names in the order they were used during model training.\n",
    "    :param feature_names: List of feature names as they were used during model training.\n",
    "    :return: Predicted category.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for the input features with the correct column names\n",
    "    input_df = pd.DataFrame([[close, volume]], columns=feature_names)\n",
    "\n",
    "    # Predict using the model\n",
    "    predictions = model.predict(input_df)[0]\n",
    "\n",
    "    # Map predictions to their corresponding target names\n",
    "    prediction_dict = dict(zip(target_names, predictions))\n",
    "\n",
    "    # Aggregate predictions\n",
    "    buy_prediction = prediction_dict['Buy_%'] + prediction_dict['Outperform_%']\n",
    "    sell_prediction = prediction_dict['Sell_%'] + prediction_dict['Underperform_%']\n",
    "    hold_prediction = prediction_dict['Hold_%']\n",
    "\n",
    "    categories = {'Buy': buy_prediction, 'Hold': hold_prediction, 'Sell': sell_prediction}\n",
    "    predicted_category = max(categories, key=categories.get)\n",
    "\n",
    "    return predicted_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Close', 'Volume']\n",
    "target_names = ['Buy_%', 'Outperform_%', 'Hold_%', 'Underperform_%', 'Sell_%']\n",
    "predicted_category = predict_analyst_category(best_model, 40, 1000000, target_names, feature_names)\n",
    "print(\"Predicted Analyst Consensus:\", predicted_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "For iteration zero we will do the feedback inside the notebook. In the next iteration when the models performs but a bit of accuracy we will create a seperate document together with a front end.\n",
    "\n",
    "Comment from stakeholder about the progress;\n",
    "It looks complex and after your explanation from what you did i can see that the data from asr is important. i am curious for the front end maybe then it will become clearer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Iteration Zero\n",
    "\n",
    "### Data Limitations\n",
    "Our analysis in phase 2 faced challenges due to recent changes in financial calculations, effective from the start of 2023. These changes limited our ability to utilize our initially identified data effectively. Furthermore, the restriction in accessing historical analyst consensus data, available only for the past 18 months, resulted in a data availability mismatch. This limitation prevented us from leveraging older, potentially valuable data sets.\n",
    "\n",
    "### Model Performance\n",
    "The baseline model underperformed, which could be attributed to insufficient optimization of the target variables and the limited size of our dataset. Additionally, the dataset lacked some key features that might be crucial in predicting the analyst consensus.\n",
    "\n",
    "### Future Iterations\n",
    "Looking ahead, we need to find ways to circumvent the financial calculation changes or access more extensive historical data for the analyst consensus. Enhancing our target variable optimization and refining the model to predict these targets more accurately will be a focus. Addressing potential biases and improving model performance may become feasible with a more comprehensive dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
