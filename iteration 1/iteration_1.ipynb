{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Iteration 1\n",
    "- **Objective:** The primary focus is to expand the dataset with new financial data from asr, improve model performance and incorporate feedback\n",
    "- **Outcome**\n",
    "    - **Data Enhancement:** Added the new financial data from asr\n",
    "    - **Model Evolution:** Switched to time series rolling cross validation\n",
    "    - **Parameter Optimization:** New parameters where selected due to addition of new financial data from asr\n",
    "    - **General:** Model improved MSE and MAE, also the model become more transparant and is better optimized for the small dataset\n",
    "\n",
    "## Previous iteration information\n",
    "### Iteration zero\n",
    "**Iteration Zero**\n",
    "- **Objective:** Find out if the project is viable\n",
    "- **Outcome:** The data analysis in phase 2 faced challenges due to financial calculation changes from 2023 and restricted access to historical analyst consensus data, limiting our dataset's effectiveness. The baseline model underperformed, likely due to insufficient target variable optimization and missing key predictive features. Future iterations should focus on overcoming data limitations, refining target optimization, and expanding the dataset to enhance model performance and mitigate biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"pandas version: \", pd.__version__)\n",
    "print(\"seaborn version:\", sns.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Requirements\n",
    "This data dictionary is for all data that will be used for our model. We have taken this out of the data requirement document (this can be found in this folder under the name data_requirements.docx), A changelog will be kept in the data requirement document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "| Dutch Term              | English Term             | Description                                                        | Data Type      | Range                        | Units          | Source          | Quality Standards          |\n",
    "|-------------------------|--------------------------|--------------------------------------------------------------------|----------------|------------------------------|----------------|-----------------|----------------------------|\n",
    "| Datum                   | Date                     | The date when the analyst ratings were recorded.                   | Date           | 2022-07-14 to 2022-12-01     | Date           | Marketscreener     | Accurate date required     |\n",
    "| Kopen                   | Buy                      | Number of 'Buy' ratings from analysts.                             | Numerical      | 0 - 15                       | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Overpresteren           | Outperform               | Number of 'Outperform' ratings from analysts.                      | Numerical      | 0 - 15                       | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Houden                  | Hold                     | Number of 'Hold' ratings from analysts.                            | Numerical      | 0 - 15                       | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Onderpresteren          | Underperform             | Number of 'Underperform' ratings from analysts.                    | Numerical      | 0 - 15                            | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Verkopen                | Sell                     | Number of 'Sell' ratings from analysts.                            | Numerical      | 0 - 15                            | Ratings        | Marketscreener     | Accurate count             |\n",
    "| Datum | Date         | The date when the stock market data was recorded.                           | Date       | Varied (min 11-06-2016 etc.)  | Date     | Yahoo Finance| Accurate date required   |\n",
    "| Open | Open         | Opening price of the stock for the given date.                              | Numerical  | Varied (min 0,.-max ∞)                    | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Hoog | High         | Highest price of the stock reached on the given date.                       | Numerical  | Varied (min 0,.-max ∞)                   | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Laag | Low          | Lowest price of the stock on the given date.                                | Numerical  | Varied (min 0,.-max ∞)                   | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Slot | Close        | Closing price of the stock at the end of the trading day on the given date. | Numerical  | Varied (min 0,.-max ∞)                   | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Aangpaste slot | Adj Close    | Adjusted closing price after adjustments for all applicable splits and dividend distributions.| Numerical | Varied (min 0,.-max ∞) | Currency | Yahoo Finance| Accurate financial data  |\n",
    "| Volume | Volume       | Number of shares of the stock traded during the given date.                 | Numerical  | Varied (min 0,.-max ∞)                   | Shares   | Yahoo Finance| Accurate count           \n",
    "| Datum       | Date    | The date from the financial report.    | Date   | 2022-01-01 to 2023-01-01  | Date   | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)   | Accurate date required     \n",
    "| Eigen Vermogen Rendement (%)| Return On Equity (%)| income level a firm is generating as a percentage of shareholder's equity| Numerical| 11.3 - 15.3 | Percent | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)  | Accurate percentage        \n",
    "| Andere uitgebreide inkomsten | Other Comprehensive Income           | Total comprehensive income not reported in the profit or loss.     | Numerical    | -874000000 - 562000000        | Currency       | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)   | Accurate financial data    \n",
    "| Gecombineerde Ratio (%)  P&C and Arbeidsongeschiktheid   | Combined Ratio P&C and Disability (%)| The combined ratio for property and casualty, and disability insurance sectors. | Numerical    | 91.7 - 92.8                  | Percent        | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)   | Accurate percentage        \n",
    "| Uitstaande Aandelen (gewogen gemiddelde)| Outstanding Shares (Weighted Average) | Weighted average of outstanding shares during the period.         | Numerical    | 134999182.84 - 137004579.72  | Shares         | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)   | Accurate count \n",
    "| Publicatie                      | Publication                          | The financial period the data represents    | Text            | e.g., FY2021, HY2022, FY2022 | N/A            | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)   | Accurate financial period    |\n",
    "| Bestandsnaam1                   | Filename1                            | The name of the first file from ASR where the data was sourced.    | Text            | Varied                       | N/A            | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)   | Accurate file reference      |\n",
    "| Bestandsnaam2                   | Filename2                            | The name of the second file from ASR where the data was sourced.   | Text            | Varied                       | N/A            | [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)   | Accurate file reference      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Analyst consensus**: Extracted by typing over the data from an interactive graph, Currently there are no free sources to get this data.\n",
    "- **Financial data from ASR**: Looked through the financials from ASR on their website, then extracted the neccessary data by typing it over in a new excel file. This data is collected pre-IFRS 17/9\n",
    "- **Stock data for ASR**: On the Yahoo website there is a page to download the data by Day, Month or Year. The data range can be stated before downloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed process\n",
    "We will explain exactly how we collected the data so these steps can be replicated.\n",
    "\n",
    "#### Analyst consensus\n",
    "- **Source**: [marketscreener.com](https://www.marketscreener.com/quote/stock/ASR-NEDERLAND-N-V-28377340/consensus/)\n",
    "- **Limitation**: Not downloadable data and only up to 18 months of history.\n",
    "- **Time Frame**: Data from 2022 July up to 2023 Januari. With a moving time frame(Only 18 months back from current date). With around 2 weeks between every entry\n",
    "- **Script**: No script used, we typed over the values from the website in to an excel file.\n",
    "- **Storage**: Data stored locally in the data folder. Filename: [analyst_consensus_16_07-2022_17-01-2024.xlsx](data/analyst_consensus_16_07-2022_17-01-2024.xlsx).\n",
    "- **Future Data Addition**: Look for a source with better historical data that is easier to gather\n",
    "\n",
    "\n",
    "#### Financial data from asr\n",
    "- **Source**: [asrnederland.nl](https://www.asrnederland.nl/investor-relations/financiele-publicaties)\n",
    "- **Limitation**: The report are per half year or full year. The best would be that a shorter time frame is available. This is not likely to happen \n",
    "- **Time Frame**: Financial data available from 2016 up to 2023. (asr whent public in 2016 June). Data collected from the following publications: FY2020, HY2021, FY2021, HY2022, FY2022\n",
    "- **Script**: No script used. Downloaded the 'Tables' and 'Financial ratios' under column 'Halfjaarcijfers' in the tab from '2022' and '2023'.\n",
    "- **Storage**: Data stored locally in the data folder. Filename: [extracted_financial_data_Pre-IFRS_FY2021_HY2022_FY2022.xlsx](data/extracted_financial_data_Pre-IFRS_FY2021_HY2022_FY2022.xlsx)\n",
    "The original data from the asr website is stored in the folder named 'Original data asr'\n",
    "- **Future Data Addition**: When newer reports are released these should be downloaded and the data should be added to [extracted_financial_data_Pre-IFRS_FY2021_HY2022_FY2022.xlsx](data/extracted_financial_data_Pre-IFRS_FY2021_HY2022_FY2022.xlsx)\n",
    "\n",
    "---\n",
    "- **Important note**: We excluded the IFRS 17/9 financial data during collection. This is first recorderd from the publication HY2023. They also have recalculated metrics for HY2022 and FY2022. This data is not useful for our project at this moment in time\n",
    "---\n",
    "\n",
    "#### Stock data for asr\n",
    "- **Source**: [finance.yahoo.com](https://finance.yahoo.com/quote/ASRNL.AS/history)\n",
    "- **Limitation**: \n",
    "- **Time Frame**: Financial data available from 2016 up to 2023. (asr whent public in 2016 July)\n",
    "- **Script**: No script used. Under the tab history there is the option to download the data from a given date as daily, monthly or yearly data. The download is in a csv format.\n",
    "- **Storage**: Data stored locally in the data folder. We downloaded the full data available in three files\n",
    "  - Daily: Filename: [StockPerDay_ASRNL.AS_01-01-2016_19-01-2024.csv](data/StockPerDay_ASRNL.AS_01-01-2016_19-01-2024.csv)\n",
    "  - Weekly: Filename: [StockPerWeek_ASRNL.AS_01-01-2016_19-01-2024.csv](data/StockPerWeek_ASRNL.AS_01-01-2016_19-01-2024.csv)\n",
    "  - Yearly: Filename: [StockPerYear_ASRNL.AS_01-01-2016_19-01-2024.csv](data/StockPerYear_ASRNL.AS_01-01-2016_19-01-2024.csv)\n",
    "- **Future Data Addition**: When newer data is available for `Analyst consensus` or `Financial data from ASR` then from the yahoo source the new stock data should be downloaded and replace the existing files for daily, monthly and yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling\n",
    "All data handling, including preprocessing and cleaning, will be conducted within this notebook, focusing on highways in the Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding & Preparation for analyst consensus\n",
    "In this part we will look at the data for the analyst consensus. This will be the most important data for our target variable.\n",
    "\n",
    "We will start with loading the data and taking a first look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalystConsensus = pd.read_excel('data/analyst_consensus_16_07-2022_17-01-2024.xlsx')\n",
    "AnalystConsensus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded correctly. We can see from this;\n",
    "- one column to indicate the date\n",
    "- 5 columns to show the count of the analyst consensus\n",
    "- There seems to be around two weeks between every entry\n",
    "\n",
    "We will now describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalystConsensus.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know there are 37 entries into the dataset.\n",
    "The newest datapoint is from 17-01-2024 and the oldest is from 14-07-2022.\n",
    "\n",
    "What is also visible is that the max count of the consensus fields is in all cases below 9. From this we can see that at maximum there are not a lot of analyst giving their opinion about asr\n",
    "\n",
    "Now we will look if the types are correct with the .info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalystConsensus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that everything has the correct datatype and there are no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "Below we will create an interactive plot to visualize the data in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = AnalystConsensus\n",
    "\n",
    "# Initialize go\n",
    "fig = go.Figure()\n",
    "\n",
    "# add bars\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Sell'],\n",
    "    name='Sell',\n",
    "    marker_color='red'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Underperform'],\n",
    "    name='Underperform',\n",
    "    marker_color='orange'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Hold'],\n",
    "    name='Hold',\n",
    "    marker_color='yellow'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Outperform'],\n",
    "    name='Outperform',\n",
    "    marker_color='lightgreen'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Buy'],\n",
    "    name='Buy',\n",
    "    marker_color='green'\n",
    "))\n",
    "\n",
    "# update the figure\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Analyst Consensus - Interactive Stacked Bar Chart',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Consensus Count',\n",
    "    legend_title='Consensus',\n",
    "    hovermode='x'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that analust overall think that asr is a good buy. With a slight hiccup around march where there was a sell consensus.\n",
    "\n",
    "We can also see that there is a difference in the count over the weeks, this is something to look into in the prepocessing step of Phase 3 as this can influence the machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The data for the analyst consensus did not need any altering. The data is clear and we have a good visualization that shows the consensus over time, what we did notice it the varying count over the weeks. This is something we need to look at in the preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step\n",
    "Find out what happened in and around march that negatively infleanced the analyst consensus to ``sell``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding & Preparation for Financial data from asr\n",
    "In this section we will look at the financial data gathered from asr.\n",
    "\n",
    "We will start with loading the data and taking a first look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinancialDataASR = pd.read_excel('data/extracted_financial_data_Pre-IFRS_FY2021_HY2022_FY2022.xlsx')\n",
    "FinancialDataASR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the date `2021-01-01` and `2021-06-30` not all entries are filled in. \n",
    "Due to the way we collected the data we know that the OCI for publication `HY2021` was not available in the report from asr. For the publication `FY2020` we explicitly left fields open due to time constraint.\n",
    "\n",
    "In this iteration we will choose to remove the first two fields. We will do this below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinancialDataASR = FinancialDataASR[~FinancialDataASR['publication'].isin(['FY2020', 'HY2021'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the code executed we will use an assert to check if the values are succesfully removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not FinancialDataASR['publication'].isin(['FY2020', 'HY2021']).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error is thrown. We will reset the index and check if the datatypes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinancialDataASR.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinancialDataASR.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datatypes are correct. Since the dataset is so small at this point we will not use the .describe. All information is visible in the .head() and with that visual and our knowledge about the data collection, no strange values are seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "Since the dataframe only contains 3 rows we will not create any visualization as this does not add anything to our understanding. From looking at the dataframe we can get a good sense of what is in the data and the connection between the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding & Preparation for Stock data for asr\n",
    "In this section we will look at the stock data gathered for asr.\n",
    "\n",
    "The analyst consensus data has a new entry around every two weeks. We want to have our data matching to this timeframe. For the stock data we will use the data gathered per week.\n",
    "We will start with loading the data and taking a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek = pd.read_csv('data/StockPerWeek_ASRNL.AS_01-01-2016_19-01-2024.csv', delimiter=',', decimal='.')\n",
    "StockPerWeek.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataframe columns for the date. We can see that the data is indeed weekly since every 7 days there is a new entry.\n",
    "\n",
    "The dataframe also contains data for the Open, High, Low and Volume of that week.\n",
    "\n",
    "The *Close and **Adj Close are fields with a description. This is the explanation from Yahoo;\n",
    "- `*Close price adjusted for splits.`\n",
    "- `**Adjusted close price adjusted for splits and dividend and/or capital gain distributions.`\n",
    "\n",
    "In the next step we will use .info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 397 entries for all columns and we can also see that there are no null values.\n",
    "\n",
    "for the datatypes we need to change the data from object to a date, we will do this in the code block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime\n",
    "StockPerWeek['Date'] = pd.to_datetime(StockPerWeek['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the .info above we can see that the conversion of `Date` was succesful.\n",
    "Now we will use .describe to look for anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .describe shows that the dataset appears normal, with no extreme values observed in any of the columns. One observation is that the earliest date in the dataset is June 13, 2016. This date represents more historical data than is currently required for matching with the analyst consensus.\n",
    "\n",
    "Going forward, our plan includes the integration of additional analyst consensus data. This means that we will review this entire dataset, starting from 2016. This ensures that we can effectively incorporate new consensus data as it becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "Below we will start the visualization with creating a plot to visualize the volume over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "df = StockPerWeek\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['Date'], df['Volume'], color='blue', label='Trading Volume', width=3)\n",
    "plt.title('Trading Volume Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "\n",
    "# This is needed to transform the mathemetical notation to numbers\n",
    "plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "\n",
    "# Set the limit to 8.000.000 to capture most datapoint and still have it readable\n",
    "plt.ylim(0, 8000000)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided plot, which appears to show the trading volume over time, we can observe that there is a fluctuating but somewhat consistent pattern of trade volume throughout the period from 2017 to 2024. There are notable spikes in volume at certain intervals, which could correspond to specific market events, earnings announcements, or other news affecting trading activity. However, the majority of trading days seem to have a volume well below these peaks. The consistency of the lower volume suggests a baseline level of trading activity, while the spikes indicate periods of high trader interest or market volatility. It would be interesting to cross-reference the spikes with actual market events to draw more detailed conclusions on what may have caused these surges in trading volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to see the close price over time as this gives us a good overview of the stock movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Date'], df['Close'], label='Close Price')\n",
    "plt.title('Closing Stock Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to see the two plots overlapping with each other to see if there is an effect on the close price due to trading volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the first plot for trading volume on the primary y-axis\n",
    "ax1 = plt.gca()  # Get the current Axes instance on the current figure\n",
    "ax1.bar(df['Date'], df['Volume'], color='blue', label='Trading Volume', width=3)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Volume', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "ax1.set_ylim(0, 8000000)  # Set the limit to capture most datapoints for volume\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Create a secondary y-axis for the closing price\n",
    "ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "ax2.plot(df['Date'], df['Close'], label='Close Price', color='green')\n",
    "ax2.set_ylabel('Price', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('Trading Volume and Closing Stock Prices Over Time')\n",
    "ax1.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not appear that there is a direct correlation between the stock price movement and trading volume.\n",
    "\n",
    "It is noteworthy that there are a few occasions where the stock price drops dramatically and then we can also see a massive spike in trading volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating data from Stock market and Analyst consensus\n",
    "We know from our domain understanding that the consensus is often reflected in the stock market or the other way around.\n",
    "\n",
    "In this section we will add the datasets together, we will lose a part of the stock market data by doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will add a column to both dataset to state the number of the week and one column for the year, afterards we will join them based on the week number.\n",
    "We will start with the analyst consensus dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnalystConsensus\n",
    "\n",
    "# Creating a column named Week to store the week number\n",
    "AnalystConsensus['Week'] = AnalystConsensus['Date'].dt.isocalendar().week\n",
    "\n",
    "# Creating a column named Year to store the year number\n",
    "AnalystConsensus['Year'] = AnalystConsensus['Date'].dt.year\n",
    "\n",
    "# Display the first rows to verify\n",
    "AnalystConsensus.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the ``AnaylstConsensus`` is done we will do the same for the ``StockPerWeek`` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StockPerWeek\n",
    "\n",
    "# Creating a column named Week to store the week number\n",
    "StockPerWeek['Week'] = StockPerWeek['Date'].dt.isocalendar().week\n",
    "\n",
    "# Creating a column named Year to store the year number\n",
    "StockPerWeek['Year'] = StockPerWeek['Date'].dt.year\n",
    "\n",
    "# Display the first rows to verify\n",
    "StockPerWeek.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we added the week column we need to edit the stock market dataframe.\n",
    "\n",
    "First we will look at the max and min week + year of the analyst consensus dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum years in the DataFrame\n",
    "min_year = AnalystConsensus['Year'].min()\n",
    "max_year = AnalystConsensus['Year'].max()\n",
    "\n",
    "# Filtering AnalystConsensus to only include rows from the min year\n",
    "min_year_df = AnalystConsensus[AnalystConsensus['Year'] == min_year]\n",
    "\n",
    "# Now we look for the min week within the min year\n",
    "min_week_in_min_year = min_year_df['Week'].min()\n",
    "\n",
    "# Here we will do the same for max\n",
    "max_year_df = AnalystConsensus[AnalystConsensus['Year'] == max_year]\n",
    "max_week_in_max_year = max_year_df['Week'].max()\n",
    "\n",
    "\n",
    "print(f\"The minimum week in the minimum year {min_year} is: {min_week_in_min_year}\")\n",
    "print(f\"The maximum week in the maximum year {max_year} is: {max_week_in_max_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the min is week 28 of the year 2022 and the max is week 3 of the year 2024.\n",
    "\n",
    "We will remove all fields that do not fall inbetween these weeks from the stock market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include rows that fall within the given range\n",
    "# Include all weeks in the min_year after min_week_in_min_year\n",
    "# Include all weeks in the max_year up to and including max_week_in_max_year\n",
    "# For years between min_year and max_year, include all weeks\n",
    "\n",
    "filtered_df = StockPerWeek[\n",
    "    ((StockPerWeek['Year'] == min_year) & (StockPerWeek['Week'] >= min_week_in_min_year)) |\n",
    "    ((StockPerWeek['Year'] == max_year) & (StockPerWeek['Week'] <= max_week_in_max_year)) |\n",
    "    ((StockPerWeek['Year'] > min_year) & (StockPerWeek['Year'] < max_year))\n",
    "]\n",
    "\n",
    "# Now let's print the shape of the original and filtered DataFrames to see how many rows were removed\n",
    "print(f\"Original DataFrame shape: {StockPerWeek.shape}\")\n",
    "print(f\"Filtered DataFrame shape: {filtered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the shape we can see that changes have been made. There now should be 80 rows in the dataset. This appears to be the correct time range. We will print the max and min week + year to check if the values are the same as in the ``AnalystConsensus`` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockPerWeek = filtered_df\n",
    "\n",
    "# Find the minimum and maximum years in the DataFrame\n",
    "min_year = StockPerWeek['Year'].min()\n",
    "max_year = StockPerWeek['Year'].max()\n",
    "\n",
    "# Filtering AnalystConsensus to only include rows from the min year\n",
    "min_year_df = StockPerWeek[StockPerWeek['Year'] == min_year]\n",
    "\n",
    "# Now we look for the min week within the min year\n",
    "min_week_in_min_year = min_year_df['Week'].min()\n",
    "\n",
    "# Here we will do the same for max\n",
    "max_year_df = StockPerWeek[StockPerWeek['Year'] == max_year]\n",
    "max_week_in_max_year = max_year_df['Week'].max()\n",
    "\n",
    "\n",
    "print(f\"The minimum week in the minimum year {min_year} is: {min_week_in_min_year}\")\n",
    "print(f\"The maximum week in the maximum year {max_year} is: {max_week_in_max_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that the min and max values we will perform one last check. See if the weeknumbers + the year are unique in each datasets. This way we can check if any data was incorrectly changed during the transformation steps above, **as an example; one weeknumber twice in the dataset is wrong**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrames\n",
    "df1 = pd.DataFrame(StockPerWeek)\n",
    "df2 = pd.DataFrame(AnalystConsensus)\n",
    "\n",
    "# Combine 'Week' and 'Year' into a single string for each DataFrame\n",
    "df1['WeekYear'] = df1['Week'].astype(str) + df1['Year'].astype(str)\n",
    "df2['WeekYear'] = df2['Week'].astype(str) + df2['Year'].astype(str)\n",
    "\n",
    "# Convert to lists\n",
    "list1 = df1['WeekYear'].tolist()\n",
    "list2 = df2['WeekYear'].tolist()\n",
    "\n",
    "# Use assert to check if each DataFrame's 'WeekYear' column has only unique values\n",
    "try:\n",
    "    # check all number of unique values againts the numbers of items in de dataframe\n",
    "    assert df1['WeekYear'].nunique() == len(df1['WeekYear'])\n",
    "    assert df2['WeekYear'].nunique() == len(df2['WeekYear'])\n",
    "    print(\"Both DataFrames have only unique 'WeekYear' values.\")\n",
    "except AssertionError:\n",
    "    print(\"One or both DataFrames do not have only unique 'WeekYear' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that they are the same, now we can start joining the two dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two DataFrames on 'Year' and 'Week' using an inner join.\n",
    "merged_df = pd.merge(StockPerWeek, AnalystConsensus, on=['Year', 'Week'], how='inner')\n",
    "\n",
    "# This will result in a DataFrame that only contains rows where both 'Year' and 'Week' match in both DataFrames.\n",
    "# All weeks that do not exist in AnalystConsensus will be dropped.\n",
    "\n",
    "# Displaying the shape of the new df\n",
    "print(f\"Merged DataFrame shape: {merged_df.shape}\")\n",
    "\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the two dataframes are merged on week and year we will continue to Phase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have added the ``AnalystConsensus`` and `StockPerWeek` will add the last dataframe `FinancialDataASR` to the merged dataset and use linear interpolation to fill in the gaps. We will first look at the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinancialDataASR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will handle this the same as the previous dataframes. We will start by adding a column for ``Week`` and ``Year``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FinancialDataASR\n",
    "\n",
    "# Creating a column named Week to store the week number\n",
    "FinancialDataASR['Week'] = FinancialDataASR['date'].dt.isocalendar().week\n",
    "\n",
    "# Creating a column named Year to store the year number\n",
    "FinancialDataASR['Year'] = FinancialDataASR['date'].dt.year\n",
    "\n",
    "# Check the transformation\n",
    "FinancialDataASR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the code, because of isocalendat, made the first of januari week 52. This is not exactly what we want so we will tranform this value 52 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = FinancialDataASR\n",
    "\n",
    "# Create a month column\n",
    "df['Month'] = df['date'].dt.month\n",
    "\n",
    "# Find rows where 'Week' is 52 and 'Month' is January (1)\n",
    "condition = (df['Week'] == 52) & (df['Month'] == 1)\n",
    "\n",
    "# Update the 'Week' column for these rows to 1\n",
    "df.loc[condition, 'Week'] = 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was succesful, now we will drop the ``Month`` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Month'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use linear interpolation to fill in the weeks between the minimum and maximum date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fill the dataframe with empty rows for the weeks that will be filled in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the week number for specific dates\n",
    "df['Week'] = df['date'].dt.isocalendar().week\n",
    "df['Year'] = df['date'].dt.year\n",
    "df.loc[(df['date'].dt.month == 1) & (df['Week'] > 50), 'Week'] = 1\n",
    "\n",
    "# Determine the range of dates\n",
    "min_date = df['date'].min()\n",
    "max_date = df['date'].max()\n",
    "\n",
    "# Generate a complete range of dates\n",
    "all_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "df_all_dates = pd.DataFrame(all_dates, columns=['date'])\n",
    "df_all_dates['Week'] = df_all_dates['date'].dt.isocalendar().week\n",
    "df_all_dates['Year'] = df_all_dates['date'].dt.year\n",
    "\n",
    "# Adjust the week number for the all_dates DataFrame as well\n",
    "df_all_dates.loc[(df_all_dates['date'].dt.month == 1) & (df_all_dates['Week'] > 50), 'Week'] = 1\n",
    "\n",
    "# Merge the new DataFrame with the original DataFrame based on week number and year\n",
    "df_merged = df_all_dates.merge(df, on=['Week', 'Year'], how='left')\n",
    "\n",
    "# Drop duplicate weeks, keeping the first occurrence\n",
    "df_merged.drop_duplicates(subset=['Week', 'Year'], inplace=True)\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to convert types, this makes interpolation easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert extension types to standard types for interpolation\n",
    "for col in df_merged.select_dtypes(include=['category', 'Int64', 'UInt32', 'boolean']).columns:\n",
    "    df_merged[col] = df_merged[col].astype('float64')\n",
    "numeric_cols = df_merged.select_dtypes(include=[np.number])\n",
    "\n",
    "numeric_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the actual interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values for numeric columns\n",
    "df_merged[numeric_cols.columns] = numeric_cols.interpolate(method='linear')\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the values are filled in. Now we need to merge it on the analyst consensus and stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two DataFrames on 'Year' and 'Week' using an inner join.\n",
    "final_df = pd.merge(df_merged, merged_df, on=['Year', 'Week'], how='inner')\n",
    "\n",
    "# This will result in a DataFrame that only contains rows where both 'Year' and 'Week' match in both DataFrames.\n",
    "# All weeks that do not exist in AnalystConsensus will be dropped.\n",
    "\n",
    "# Displaying the shape of the new df\n",
    "print(f\"Merged DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "\n",
    "final_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is in one dataframe we will check the datatypes. We know that there might be problems with the asr financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that all datatype are correct but there is a dubplication of the date. We will remove this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'date_y' column\n",
    "final_df = final_df.drop(columns=['date_y'])\n",
    "final_df = final_df.drop(columns=['Date_y'])\n",
    "final_df = final_df.drop(columns=['Date_x'])\n",
    "\n",
    "# Rename 'date_x' to 'date'\n",
    "final_df = final_df.rename(columns={'date_x': 'date'})\n",
    "\n",
    "# Now we check the dataframe\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We will collect all imports at the top of this section to keep it clear what we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "df = final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the data we are working with using the .describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target variable is heavily influenced to the buy side. There are no values for ``Underperform`` or ``Sell``. This is something to keep in mind for the modelling.\n",
    "\n",
    "From Phase 2 we know that the total amount of the target variable switches every so often. It would be best to change these numbers to percentages based on the total count of the week. We will be doing this in the code block below step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of ratings for each row\n",
    "df['Total_Ratings'] = df[['Buy', 'Outperform', 'Hold', 'Underperform', 'Sell']].sum(axis=1)\n",
    "\n",
    "# Convert each rating to a percentage of the total\n",
    "for column in ['Buy', 'Outperform', 'Hold', 'Underperform', 'Sell']:\n",
    "    df[column + '_%'] = (df[column] / df['Total_Ratings']) * 100\n",
    "\n",
    "# Drop the original count columns and the total ratings column if they are no longer needed\n",
    "df.drop(['Buy', 'Outperform', 'Hold', 'Underperform', 'Sell', 'Total_Ratings'], axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will create a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation calculation\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "# Title for the heatmap\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before there are 0 readings for ``Underperform_%`` or ``Sell_%``. This means our data is biased to the buy side.\n",
    "\n",
    "Besides this we can see that there is a high correlation between every feature. This is good and strange, for now we will not investigate this due to time contraints\n",
    "\n",
    "From the data of the stck market we will only take the `Open` and `Adj Close`. The other values from the stock data are very closely related but we want to keep the model simple to use and in this case more features that tell kind of the same will not make a big difference in performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "From the correlation matrix above we can see that the `Open`, `High`, `Low`, `Close` and `Adj Close` are highly correlated. We will pick the `Close` feature .\n",
    "It seems that the ``Volume`` does not have a high correlation but we will keep it in there for iteration zero.\n",
    "\n",
    "It also seems that the ``Year`` has a high correlation to the target. Since we only have 1 full year we will not be using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target columns as a list of strings\n",
    "target_columns = ['Buy_%', 'Outperform_%', 'Hold_%', 'Underperform_%', 'Sell_%']\n",
    "\n",
    "# Include these target columns in your target DataFrame\n",
    "target = df[target_columns]\n",
    "\n",
    "# Define columns to exclude (including the target columns)\n",
    "columns_to_exclude = target_columns + ['Week', 'Year', 'High', 'Low', 'Close']\n",
    "\n",
    "# Select columns except the ones listed in columns_to_exclude\n",
    "selected_columns = [col for col in numeric_df.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Create a DataFrame with only the selected columns\n",
    "features = numeric_df[selected_columns]\n",
    "\n",
    "# Define your features (X) and target (y)\n",
    "X = features\n",
    "y = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting in to train/test\n",
    "Commented due to different cross validation approach<br>\n",
    "*The data we use is time series we need to keep this in mind with splitting the data. For now we will set the split at 80%.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set splitting point at 80%\n",
    "# split_point = int(len(df) * 0.80)\n",
    "\n",
    "# # Split the features and target into training and testing sets\n",
    "# X_train, X_test = X[:split_point], X[split_point:]\n",
    "# y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "# # Print the sizes of the train and test sets\n",
    "# print(f\"Training set size: {X_train.shape[0]} rows\")\n",
    "# print(f\"Test set size: {X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "In iteration zero we selected a Random Forest Regressor as our baseline model to predict weekly analyst consensus ratings, which are represented as percentages. Random Forest is chosen for its ability to handle complex and varied data types effectively. It's known for its robustness, reducing overfitting by averaging multiple decision trees.\n",
    "\n",
    "For this iteration we will still be using the random forest regressor but we will take a different approach to our cross validation technique due to our very small dataset and keeping in mind that the data is time series.\n",
    "\n",
    "We will be using Time-series cross validation, rolling cross-validation. This means we will comment out the original train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with finding the optimal hyperparameters on the entire dataset. This is especially handy in our sitation of a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [100, 200, 300],\n",
    "    'estimator__max_depth': [None, 10, 20, 30],\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "random_forest_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Wrap the model with MultiOutputRegressor\n",
    "multi_target_regressor = MultiOutputRegressor(random_forest_regressor)\n",
    "\n",
    "# Set up the grid search with cross-validation\n",
    "grid_search = GridSearchCV(multi_target_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the full dataset\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start applying the rolling cross validation to the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the metrics for each roll\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "time_indices = []\n",
    "\n",
    "initial_train_size = int(len(X) * 0.15)\n",
    "roll_size = 1  # Set to 1 for a single-step roll forward\n",
    "\n",
    "for i in range(initial_train_size, len(X) - roll_size):\n",
    "    # Define the train and test sets for the current roll\n",
    "    X_train, X_test = X[:i], X[i:i + roll_size]\n",
    "    y_train, y_test = y[:i], y[i:i + roll_size]\n",
    "\n",
    "    # Fit the best model on the current training set\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the current test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate and store the metrics for the current roll\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "    # Store predictions and actual values\n",
    "    all_predictions.append(y_pred[0])  # Assuming y_pred is a 1D array\n",
    "    all_actuals.append(y_test.iloc[0])  # Assuming y_test is a DataFrame\n",
    "    time_indices.append(i)  # Or your specific time index\n",
    "\n",
    "# Calculate the average of the metrics across all rolls\n",
    "mean_mse = np.mean(mse_scores)\n",
    "mean_mae = np.mean(mae_scores)\n",
    "\n",
    "print(f\"Rolling Cross-Validated Mean Squared Error: {mean_mse}\")\n",
    "print(f\"Rolling Cross-Validated Mean Absolute Error: {mean_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach we can see that the Mean Squared Error and Mean Absolute Error both improved.\n",
    "\n",
    "**Old** Result standard cross validation\n",
    "- Mean Squared Error: 11.034635994042576\n",
    "- Mean Absolute Error: 2.2646886446886447\n",
    "- Cross-validated MSE: 5.128838594372667\n",
    "\n",
    "**New** Results rolling cross validation\n",
    "- Rolling Cross-Validated Mean Squared Error: 10.791339472554373\n",
    "- Rolling Cross-Validated Mean Absolute Error: 1.6185091575091584\n",
    "\n",
    "The new result look better then the previous approach. With the initial train size set to .80 there is a change that we are overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to arrays for easier handling\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_actuals = np.array(all_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Assuming you have a list of target names\n",
    "target_names = ['Buy', 'outperform', 'Hold', 'Underperform', 'Sell']\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "\n",
    "# Actual vs. Predicted values\n",
    "plt.subplot(2, 1, 1)\n",
    "for i, target_name in enumerate(target_names):\n",
    "    actual_color = colors[i % len(colors)]\n",
    "    predicted_color = 'grey'\n",
    "    plt.plot(time_indices, all_actuals[:, i], label=f'Actual - {target_name}', color=actual_color, marker='o')\n",
    "    plt.plot(time_indices, all_predictions[:, i], label=f'Predicted - {target_name}', color=predicted_color, linestyle='--', marker='x')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "\n",
    "# Residuals\n",
    "plt.subplot(2, 1, 2)\n",
    "for i, target_name in enumerate(target_names):\n",
    "    residuals = all_actuals[:, i] - all_predictions[:, i]\n",
    "    plt.plot(time_indices, residuals, label=f'Residuals - {target_name}', color=colors[i % len(colors)], marker='o')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.title('Residuals Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Descriptions\n",
    "\n",
    "### Actual vs. Predicted Values\n",
    "\n",
    "The top plot displays the actual and predicted values over time/iterations for different categories: Buy, Outperform, Hold, Underperform, and Sell. Each category is represented by a unique color with actual values depicted by solid lines and predicted values by dashed lines with markers. This plot allows us to compare the model's predictions against the real-world data across various investment decisions over a period of time.\n",
    "We can see that the line for Underperform and Sell stays at the same height. This is due to there being no values for these targets in our current dataset\n",
    "\n",
    "### Residuals Over Time\n",
    "\n",
    "The bottom plot shows the residuals over time for the same categories. Residuals are the differences between actual and predicted values, indicating the error of the predictions. Consistent with the top plot, each category has a unique color. A residual value of zero would mean a perfect prediction, while any deviation from zero represents an error.When the residuals are small its better. We can see that after 4 iterations the residuals increase but after 9 iterations they all are returning to the 0 line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We think that it is not possible to optimize the model further with the current dataset.  \n",
    "\n",
    "\n",
    "In a next iteration we need to work with more data to get a better model. With the limited data this is very hard to achieve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4\n",
    "## Demonstration\n",
    "In the following section we will create the code that can be used to make a prediction for our stakeholder. After we check if it works we are exporting the model and dataframes necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_analyst_category(model, features, target_names):\n",
    "    \"\"\"\n",
    "    Predict the analyst category ('Buy', 'Hold', or 'Sell') based on;\n",
    "    'Return_On_Equity(%)', 'OCI', 'Combined_ratio_P&C_and_Disabilty(%)', 'Outstanding_shares(weighted_average)', \n",
    "    'Open', 'Adj Close', 'Volume' using the trained model.\n",
    "\n",
    "    :param model: Trained MultiOutputRegressor model.\n",
    "    :param features: dataframe that holds all features used for training the model\n",
    "    :param target_names: List of target variable names in the order they were used during model training.\n",
    "    :param feature_names: List of feature names as they were used during model training.\n",
    "    :return: Predicted category.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for the input features with the correct column names\n",
    "    input_df = features\n",
    "\n",
    "    # Predict using the model\n",
    "    predictions = model.predict(input_df)[0]\n",
    "\n",
    "    # Map predictions to their corresponding target names\n",
    "    prediction_dict = dict(zip(target_names, predictions))\n",
    "\n",
    "    # Aggregate predictions\n",
    "    buy_prediction = prediction_dict['Buy_%'] + prediction_dict['Outperform_%']\n",
    "    sell_prediction = prediction_dict['Sell_%'] + prediction_dict['Underperform_%']\n",
    "    hold_prediction = prediction_dict['Hold_%']\n",
    "\n",
    "    categories = {'Buy': buy_prediction, 'Hold': hold_prediction, 'Sell': sell_prediction}\n",
    "    predicted_category = max(categories, key=categories.get)\n",
    "\n",
    "    return predicted_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Return_On_Equity = 10\n",
    "OCI\t= 10\n",
    "Combined_ratio_PC_and_Disabilty = 10\n",
    "Outstanding_shares_weighted_average = 10\n",
    "Open = 10\n",
    "Adj_Close = 10\n",
    "Volume = 10\n",
    "\n",
    "\n",
    "values = [Return_On_Equity, OCI, Combined_ratio_PC_and_Disabilty, Outstanding_shares_weighted_average, Open, Adj_Close, Volume]\n",
    "input_data = pd.DataFrame([values], columns=features.columns)\n",
    "\n",
    "\n",
    "target_names = ['Buy_%', 'Outperform_%', 'Hold_%', 'Underperform_%', 'Sell_%']\n",
    "predicted_category = predict_analyst_category(best_model, input_data, target_names)\n",
    "print(\"Predicted Analyst Consensus:\", predicted_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model for flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(best_model, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the same structure as X_train\n",
    "df_structure = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in X_train.dtypes.items()})\n",
    "\n",
    "# Save the structure\n",
    "df_structure.to_pickle('df_structure.pkl')\n",
    "\n",
    "df_structure.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "See file named demonstration_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Iteration 1\n",
    "*Date: 25/01/2024*<br>\n",
    "During this iteration it become clear that with the limited availability in history of the analyst consensus and limited availability of recent data from asr due to a new standard named IFRS 17 / 9 it is not possible to drastically improve the model. \n",
    "\n",
    "### IFRS 17 / 9\n",
    "This new standard makes it that there is better transparency in the insurance sector. The problem is that this new standard affects all important features that we need according to our domain understanding. The new regulations came in to effect on Januari 2023. Due to this we must choose to collect data after this date or before.\n",
    "\n",
    "#### After\n",
    "With the collection after Januari 2023 we only have one row of data. Asr only published one Half Year report. The Full Year report for 2023 is expected around May 2024\n",
    "\n",
    "#### Before\n",
    "When we look before Januari 2023 we can go back as far as 2016, on this date asr went public. The problem with collecting information before januari 2023 is the data for the Analyst consensus.\n",
    "\n",
    "### Analyst consensus\n",
    "The anaylyst consensus is currently collected from a website with the url marketwatcher.com. They offer the longest history of 18 months back with a moving time frame. That means the 18 months back is always from the current date. This means the data collection can not be earlier then June 2022. There are no other free soruces available that offer a longer history.\n",
    "Because of this we can not use the data from asr before 2022.\n",
    "\n",
    "### Bias with current data\n",
    "Another problem is the current bias in the data. Currently there are no underperform or sell values in our dataset. This makes it that the model can not predict these values and will always give the advice to either buy or hold.\n",
    "\n",
    "### Possible solution\n",
    "#### Multiple insurance companies\n",
    "To continue with a next sprint we would need more data, from asr we can not get more data. We could only look at different companies in the same sector like NN, Achmea, or Allianz. When we collect the same data from these companies then there is more data to work with.\n",
    "\n",
    "#### Paid data\n",
    "Another option would be to pay for the data, this would solve most problems. Anotrher upside would also be that data can be more easily collected and this would speed up each following iteration.\n",
    "\n",
    "\n",
    "## Older Iterations, Notebook ends here\n",
    "### Iteration Zero\n",
    "\n",
    "#### Data Limitations\n",
    "Our analysis in phase 2 faced challenges due to recent changes in financial calculations, effective from the start of 2023. These changes limited our ability to utilize our initially identified data effectively. Furthermore, the restriction in accessing historical analyst consensus data, available only for the past 18 months, resulted in a data availability mismatch. This limitation prevented us from leveraging older, potentially valuable data sets.\n",
    "\n",
    "#### Model Performance\n",
    "The baseline model underperformed, which could be attributed to insufficient optimization of the target variables and the limited size of our dataset. Additionally, the dataset lacked some key features that might be crucial in predicting the analyst consensus.\n",
    "\n",
    "#### Future Iterations\n",
    "Looking ahead, we need to find ways to circumvent the financial calculation changes or access more extensive historical data for the analyst consensus. Enhancing our target variable optimization and refining the model to predict these targets more accurately will be a focus. Addressing potential biases and improving model performance may become feasible with a more comprehensive dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
